{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/matthieulucchesi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_len: 999\n",
      "max_len: 1000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from dataset import LargeMovieDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from gensim.models import Word2Vec\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "#MODEL\n",
    "\n",
    "\n",
    "\n",
    "# class CustomRNN(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_size, num_layers, bidirectional=False):\n",
    "#         super(CustomRNN, self).__init__()\n",
    "#         self.num_layers = num_layers\n",
    "#         self.bidirectional = bidirectional\n",
    "#         self.rnn = nn.RNN(input_size, hidden_size, num_layers, bidirectional=bidirectional, batch_first=True)\n",
    "#         self.layer_norm = nn.LayerNorm(hidden_size * 2 if bidirectional else hidden_size)\n",
    "\n",
    "#     def forward(self, x, lengths):\n",
    "#         packed_seq = pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n",
    "#         output, hidden = self.rnn(packed_seq)\n",
    "#         output, _ = pad_packed_sequence(output, batch_first=True)\n",
    "#         output = self.layer_norm(output)\n",
    "#         return output, hidden\n",
    "    \n",
    "\n",
    "\n",
    "# n_hidden = 128\n",
    "# rnn = RNN(n_letters, n_hidden, n_categories)\n",
    "\n",
    "#DATALOADER\n",
    "word_embedding_size = 16\n",
    "word2vec_model = Word2Vec.load(\"../models/word2vec_model.model\")\n",
    "\n",
    "train_set = LargeMovieDataset(path=\"../data/aclImdb/\", set=\"train\", embedding_dic=word2vec_model.wv, word_embedding_size=word_embedding_size)\n",
    "test_set = LargeMovieDataset(path='../data/aclImdb/', set='test', embedding_dic=word2vec_model.wv, word_embedding_size=word_embedding_size)\n",
    "\n",
    "train_set, val_set = torch.utils.data.random_split(train_set, [int(0.8 * len(train_set)), len(train_set) - int(0.8 * len(train_set))])\n",
    "\n",
    "train_dataloader = DataLoader(train_set, batch_size=32, shuffle=True)\n",
    "val_dataloader = DataLoader(val_set, batch_size=32, shuffle=True)\n",
    "test_dataloader = DataLoader(test_set, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layer):\n",
    "        super(Model, self).__init__()\n",
    "        self.input_size = input_size        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layer = num_layer\n",
    "        self.c1 = nn.RNN(self.input_size, self.hidden_size, num_layers=num_layer, batch_first=True)\n",
    "        self.c2 = nn.Linear(self.hidden_size, 2)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x, hiden = self.c1(x)\n",
    "        x = x[:, -1, :]\n",
    "        x = self.c2(x)\n",
    "        return F.relu(x)\n",
    "    \n",
    "\n",
    "hyp_hidensize = 128\n",
    "num_layer = 1\n",
    "rnn_model = Model(word_embedding_size, hyp_hidensize, num_layer)\n",
    "\n",
    "learnong_rate=1e-2\n",
    "optimizer_rnn = torch.optim.Adam(rnn_model.parameters(), lr=learnong_rate)\n",
    "criterion_rnn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu to train\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[32m██████████\u001b[0m| 625/625 [02:14<00:00,  4.65batch/s, acc=25.00%, epoch=0, loss=0.787, set=train]\n",
      "100%|\u001b[34m██████████\u001b[0m| 157/157 [00:11<00:00, 13.28batch/s, acc=66.67%, epoch=0, loss=0.695, set=val]\n",
      "100%|\u001b[32m██████████\u001b[0m| 625/625 [02:19<00:00,  4.49batch/s, acc=62.50%, epoch=1, loss=0.663, set=train]\n",
      "100%|\u001b[34m██████████\u001b[0m| 157/157 [00:11<00:00, 13.84batch/s, acc=33.33%, epoch=1, loss=0.698, set=val]\n"
     ]
    }
   ],
   "source": [
    "from utils import train\n",
    "\n",
    "train(model=rnn_model, train_dataloader=train_dataloader, val_dataloader=val_dataloader, optimizer=optimizer_rnn, criterion=criterion_rnn, num_epochs=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
